远程衰减；

Attention Mask；

窗口外注意力截断；

Sliding Window?

【大语言模型】LongLoRA:大语言模型长文本的高效微调方法 https://zhuanlan.zhihu.com/p/658067243

$$LoRA=W_0x+\triangle Wx$$

LoRA=W0+deltaW=W0+BA

A=(rxd+(r<d)+(B))

B=(dxr)

Attention的计算复杂度=O(n^2\*d)=如果把向量维度d 看作常数，则可以说self-*attention* 的*时间复杂度*是序列长度的平方

~~~
LongLoRA=swin-transfomer+shift short attention+mask
长上下文=
对比学习=

~~~

self-attention



长上下文是指train short, inference long.

=训练+微调+缩放+高频外推+低频内插

长度外推性 https://spaces.ac.cn/archives/9431

长度的外推性是我们一直在追求的良好性质，它是指我们在短序列上训练的模型，能否不用微调地用到长序列上并依然保持不错的效果。



在这里，“效果”指的是经过短序列训练的LLM在长序列上的表现，具体体现在以下几个方面：

### 长序列训练效果

- **性能提升**：通过特定的训练方法，如LongPO，模型在长上下文任务中的性能显著提升。例如，LongPO方法在长上下文任务中超越了传统的监督微调（SFT）和直接偏好优化（DPO）方法，性能提升超过10分。
- **训练效率**：一些方法如YaRN在训练效率方面展现出显著优势。例如，YaRN在将LLaMA 7B模型的上下文窗口扩展至32k时，仅需400个训练步骤即可达到理想效果，大幅领先于其他方法。
- **长序列处理能力**：某些模型在处理超长文本序列时表现卓越。

### 长上下文性质

- **上下文长度扩展**：模型能够处理更长的上下文长度。
- **上下文并行处理**：通过序列并行方式，如Ring Attention，模型能够在多数阶段采用Packing的变长训练，提高计算效率。
- **长上下文中的检索与推理能力**：模型在长上下文中能够准确检索和推理关键信息。



评测指标：文本困惑度；一般遇到简称PPL的时候就是指Perplexity；如果训练有问题，PPL会爆炸；



在直觉上，相信很多读者觉得像[Sinusoidal](https://spaces.ac.cn/archives/8231)或[RoPE](https://spaces.ac.cn/archives/8265)之类的函数式位置编码，它们没有训练参数，长度外推性应该很好才对，但事实上并非如此，这类位置编码并没有在长度外推方面表现出什么优势。[SU]

**位置编码函数**

